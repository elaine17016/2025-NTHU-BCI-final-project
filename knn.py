import numpy as np
import h5py
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA

# === è¼‰å…¥è³‡æ–™ ===
file = h5py.File('MI_2_7-30hz.mat', 'r')  # æ›¿æ›æˆä½ çš„æª”å
X = np.array(file['X'])  # (576, 3, 500)
X = X.transpose(2, 1, 0)  # â†’ (576, 500, 3)
X = X.reshape(-1, 500)  # (576 Ã— 3, 500) â†’ (1728, 500)

y = np.array(file['y']).flatten()  # (576,)
# æ¨™ç±¤ä¹Ÿè¦è¤‡è£½ä¸‰æ¬¡ï¼Œè®“æ¯å€‹ channel å°æ‡‰æ­£ç¢º label
y = np.repeat(y, 3)  # 576 â†’ 1728
print(f'X shape: {X.shape}, y shape: {y.shape}')

# === åˆ†å‰²ç‚º training-validation & testing ===
X_trainval, X_test, y_trainval, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# === Cross-validation è¨­å®š ===
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
train_acc_list = []
val_acc_list = []
class_correct = defaultdict(int)
class_total = defaultdict(int)

fold = 1
for train_idx, val_idx in skf.split(X_trainval, y_trainval):
    X_train, X_val = X_trainval[train_idx], X_trainval[val_idx]
    y_train, y_val = y_trainval[train_idx], y_trainval[val_idx]

    # æ¨™æº–åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)

    # è¨“ç·´ KNN æ¨¡åž‹ï¼ˆK å¯è‡ªè¡Œèª¿æ•´ï¼‰
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train_scaled, y_train)

    # é æ¸¬
    y_train_pred = knn.predict(X_train_scaled)
    y_val_pred = knn.predict(X_val_scaled)

    train_acc = accuracy_score(y_train, y_train_pred)
    val_acc = accuracy_score(y_val, y_val_pred)

    print(f'ðŸ“˜ Fold {fold}: Train Acc = {train_acc:.4f} | Val Acc = {val_acc:.4f}')
    train_acc_list.append(train_acc)
    val_acc_list.append(val_acc)
    fold += 1

    # æ¯é¡žåˆ¥æ­£ç¢ºçŽ‡çµ±è¨ˆ
    for true, pred in zip(y_val, y_val_pred):
        class_total[true] += 1
        if true == pred:
            class_correct[true] += 1

# === ä½¿ç”¨å…¨éƒ¨è¨“ç·´è³‡æ–™é‡æ–°è¨“ç·´ä¸¦æ¸¬è©¦ ===
scaler_final = StandardScaler()
X_trainval_scaled = scaler_final.fit_transform(X_trainval)
X_test_scaled = scaler_final.transform(X_test)

knn_final = KNeighborsClassifier(n_neighbors=5)
knn_final.fit(X_trainval_scaled, y_trainval)

y_test_pred = knn_final.predict(X_test_scaled)
test_acc = accuracy_score(y_test, y_test_pred)

# === é¡¯ç¤ºçµæžœ ===
print("\nâœ… Cross-Validation Summary")
print(f'ðŸ“Š å¹³å‡ Train Acc: {np.mean(train_acc_list):.4f}')
print(f'ðŸ§ª å¹³å‡ Val Acc: {np.mean(val_acc_list):.4f}')

print("\nðŸŽ¯ æ¯é¡žåˆ¥é©—è­‰é›†æ­£ç¢ºçŽ‡ï¼š")
label_names = ['Left Hand', 'Right Hand']
for i in [0, 1]:
    acc = class_correct[i] / class_total[i] if class_total[i] > 0 else 0
    print(f'ðŸ“¡ {label_names[i]} Acc: {acc:.4f} ({class_correct[i]}/{class_total[i]})')

print(f'\nðŸš€ æ¸¬è©¦é›†æº–ç¢ºçŽ‡: {test_acc:.4f}')
print("\nðŸ“‹ æ¸¬è©¦é›†åˆ†é¡žå ±å‘Š:")
print(classification_report(y_test, y_test_pred, target_names=label_names))

# =========================
# ä»¥ä¸‹ç‚ºæ–°å¢žè¦–è¦ºåŒ–ç¨‹å¼ç¢¼
# =========================

# 1. è¨“ç·´èˆ‡é©—è­‰æº–ç¢ºçŽ‡æŠ˜ç·šåœ–
plt.figure(figsize=(8, 5))
plt.plot(range(1, 6), train_acc_list, marker='o', label='Training Accuracy')
plt.plot(range(1, 6), val_acc_list, marker='s', label='Validation Accuracy')
plt.title('Cross-Validation Accuracy per Fold')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.xticks(range(1, 6))
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 2. æ··æ·†çŸ©é™£ (Confusion Matrix)
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)
plt.title('Confusion Matrix on Test Set')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

# 3. æ¯é¡žåˆ¥é©—è­‰æº–ç¢ºçŽ‡é•·æ¢åœ–
val_class_acc = [class_correct[i] / class_total[i] if class_total[i] > 0 else 0 for i in [0, 1]]

plt.figure(figsize=(6, 4))
sns.barplot(x=label_names, y=val_class_acc)
plt.title('Per-Class Validation Accuracy')
plt.ylim(0, 1)
plt.ylabel('Accuracy')
plt.tight_layout()
plt.show()

# 4. æ¸¬è©¦é›†åˆ†é¡žå ±å‘Šåœ–ï¼ˆPrecision / Recall / F1ï¼‰
report = classification_report(y_test, y_test_pred, target_names=label_names, output_dict=True)
metrics = ['precision', 'recall', 'f1-score']

plt.figure(figsize=(10, 4))
for i, metric in enumerate(metrics):
    plt.subplot(1, 3, i+1)
    scores = [report[cls][metric] for cls in label_names]
    sns.barplot(x=label_names, y=scores)
    plt.title(metric.capitalize())
    plt.ylim(0, 1)
    plt.xticks(rotation=45)
    plt.tight_layout()

plt.suptitle("Classification Report Metrics (Test Set)", fontsize=14, y=1.05)
plt.tight_layout()
plt.show()

# 5. PCA äºŒç¶­è¦–è¦ºåŒ–
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_test_scaled)

plt.figure(figsize=(6, 5))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_test_pred, style=y_test, palette='Set2')
plt.title('PCA Projection of Test Set')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.legend(title='Predicted / True', loc='best')
plt.tight_layout()
plt.show()